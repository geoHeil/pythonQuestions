{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy duplicate detection\n",
    "fingerprinting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two strings can easily be compared using https://github.com/seatgeek/fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    75\n",
       "1    33\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.DataFrame({'one': ['fuzz', 'wuzz'], 'two': ['fizz', 'woo']})\n",
    "\n",
    "d.apply(lambda s: fuzz.partial_ratio(s['one'], s['two']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if I want to perform such a operation with each row of a dataframe vs. all the other rows e.g. find duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bank</th>\n",
       "      <th>date</th>\n",
       "      <th>email</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBAN1</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>first@email.com</td>\n",
       "      <td>1</td>\n",
       "      <td>name1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IBAN2</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>1@email.com</td>\n",
       "      <td>2</td>\n",
       "      <td>name1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBAN1</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>iTrickYouAndStealIBAN1sBank@other.com</td>\n",
       "      <td>3</td>\n",
       "      <td>name3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bank        date                                  email id   name\n",
       "0  IBAN1  2016-01-01                        first@email.com  1  name1\n",
       "1  IBAN2  2016-01-02                            1@email.com  2  name1\n",
       "2  IBAN1  2016-01-02  iTrickYouAndStealIBAN1sBank@other.com  3  name3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.DataFrame({'id': ['1', '2', '3'], 'email': ['first@email.com', '1@email.com', 'iTrickYouAndStealIBAN1sBank@other.com'], 'bank': ['IBAN1', 'IBAN2', 'IBAN1'], 'name': ['name1', 'name1', 'name3'], 'date': ['2016-01-01', '2016-01-02', '2016-01-02']})\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get each row as a string. Or would you recommend to calculate the distance per field and add them up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBAN1,2016-01-01,first@email.com,1,name1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IBAN2,2016-01-02,1@email.com,2,name1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBAN1,2016-01-02,iTrickYouAndStealIBAN1sBank@o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0           IBAN1,2016-01-01,first@email.com,1,name1\n",
       "1               IBAN2,2016-01-02,1@email.com,2,name1\n",
       "2  IBAN1,2016-01-02,iTrickYouAndStealIBAN1sBank@o..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = d.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "vals = pd.DataFrame([','.join(ele.split()) for ele in x])\n",
    "vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers here do not work out e.g. index 2 / id3 should be a duplicate with 1. Ideally certain columns e.g. bank would be weighted like http://stackoverflow.com/questions/19964546/python-pandas-fuzzy-merge-match-with-duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vals.apply(lambda s: fuzz.partial_ratio(s, vals), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, at least the numers work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vals.apply(lambda s: fuzz.token_set_ratio(s, vals), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "How to find duplicates of one column vs. all the other ones without a gigantic for loop of converting row_i toString() and then comparing it to all the other ones?\n",
    "\n",
    "Use the date column and add and additional column to `d` which describes *daysSinceLastPurchase*, which e.g. for *id 3* should be `today - 2016-01-01` because the \"real last purches\" occured just a day before\n",
    "\n",
    "To fill the min-hash I need to loop over all rows and all fields n*k * n to add all the sets to the LSH.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# after the initial question -> BK tree\n",
    "https://gist.github.com/nibogd/94363e93f4e0256b4665eb743dbfa211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from fuzzywuzzy import fuzz\n",
    "#import numpy as np\n",
    "#from sklearn.cluster import dbscan\n",
    "\n",
    "#def metric(x, y):\n",
    "#    i, j = int(x[0]), int(y[0])     \n",
    "#    return 1 - fuzz.ratio((vals[i], vals[j]) / 100.)\n",
    "\n",
    "#X = np.arange(len(vals)).reshape(-1, 1)\n",
    "#dbscan(X, metric=metric, eps=5, min_samples=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try lsh / min-hash\n",
    "how to efficiently loop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index,  0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-5126dcf9c067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# take everything, TODO try selected attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcurrCust\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# add filled min-hash to LSH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2670\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "# create as many min-hash's as needed\n",
    "allCustomers = []\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "\n",
    "for index, row in d.iterrows():\n",
    "    \n",
    "    print(\"index, \", index)\n",
    "    currCust = MinHash(num_perm=128)\n",
    "    for column in d.columns:\n",
    "        # add all the columns to min-hash\n",
    "        \n",
    "        # take everything, TODO try selected attributes\n",
    "        currCust.update(d[column].encode('utf8'))\n",
    "        \n",
    "    # add filled min-hash to LSH\n",
    "    allCustomers.append(currCust)\n",
    "    lsh.insert(index, currCust)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Candidates with Jaccard similarity > 0.5 ['m2', 'm3']\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "data1 = ['minhash', 'is', 'a', 'probabilistic', 'data', 'structure', 'for',\n",
    "        'estimating', 'the', 'similarity', 'between', 'datasets']\n",
    "data2 = ['minhash', 'is', 'a', 'probability', 'data', 'structure', 'for',\n",
    "        'estimating', 'the', 'similarity', 'between', 'documents']\n",
    "data3 = ['minhash', 'is', 'probability', 'data', 'structure', 'for',\n",
    "        'estimating', 'the', 'similarity', 'between', 'documents']\n",
    "\n",
    "# Create MinHash objects\n",
    "m1 = MinHash(num_perm=128)\n",
    "m2 = MinHash(num_perm=128)\n",
    "m3 = MinHash(num_perm=128)\n",
    "for d in data1:\n",
    "    m1.update(d.encode('utf8'))\n",
    "for d in data2:\n",
    "    m2.update(d.encode('utf8'))\n",
    "for d in data3:\n",
    "    m3.update(d.encode('utf8'))\n",
    "\n",
    "# Create an MinHashLSH index optimized for Jaccard threshold 0.5,\n",
    "# that accepts MinHash objects with 128 permutations functions\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "\n",
    "# Insert m2 and m3 into the index\n",
    "lsh.insert(\"m2\", m2)\n",
    "lsh.insert(\"m3\", m3)\n",
    "\n",
    "# Check for membership using the key\n",
    "print(\"m2\" in lsh)\n",
    "print(\"m3\" in lsh)\n",
    "\n",
    "# Using m1 as the query, retrieve the keys of the qualifying datasets\n",
    "result = lsh.query(m1)\n",
    "print(\"Candidates with Jaccard similarity > 0.5\", result)\n",
    "\n",
    "# Remove key from lsh\n",
    "lsh.remove(\"m2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
